{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpvIKnx6wVCXsm4kIlgKBQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RazvanPorojan/adventures-in-ml-code/blob/master/keras_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATbxpE_H6hoW",
        "colab_type": "text"
      },
      "source": [
        "Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shIyiZvo6imJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Reshape, merge\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import dot\n",
        "\n",
        "import urllib.request\n",
        "import collections\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def maybe_download(filename, url, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
        "    statinfo = os.stat(filename)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified', filename)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "    return filename\n",
        "\n",
        "\n",
        "# Read the data into a list of strings.\n",
        "def read_data(filename):\n",
        "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCtf4Nxw619O",
        "colab_type": "text"
      },
      "source": [
        "Build DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5vgG68t685T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            index = dictionary[word]\n",
        "        else:\n",
        "            index = 0  # dictionary['UNK']\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XpIGFDJ3tU1",
        "colab_type": "text"
      },
      "source": [
        "Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q93TNZlh3oSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_data(vocabulary_size=10000):\n",
        "    url = 'http://mattmahoney.net/dc/'\n",
        "    filename = maybe_download('text8.zip', url, 31344016)\n",
        "    vocabulary = read_data(filename)\n",
        "    print(vocabulary[:7])\n",
        "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
        "                                                                vocabulary_size)\n",
        "    del vocabulary  # Hint to reduce memory.\n",
        "    return data, count, dictionary, reverse_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QaDmuoa3xer",
        "colab_type": "text"
      },
      "source": [
        "Print Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlEuE09a31gk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2d756c6d-69c1-411c-be57-aa7111f268cc"
      },
      "source": [
        "vocab_size = 10000\n",
        "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
        "print(data[:7])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified text8.zip\n",
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
            "[5234, 3081, 12, 6, 195, 2, 3134]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvye1NYV4NSw",
        "colab_type": "text"
      },
      "source": [
        "Do stuff for long time..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz759rPF4PXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fc15edf-bdbd-4d13-d3e3-20dae55d51ff"
      },
      "source": [
        "window_size = 3\n",
        "vector_dim = 300\n",
        "epochs = 200000\n",
        "\n",
        "valid_size = 16     # Random set of words to evaluate similarity on.\n",
        "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "\n",
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "word_target = np.array(word_target, dtype=\"int32\")\n",
        "word_context = np.array(word_context, dtype=\"int32\")\n",
        "\n",
        "print(couples[:10], labels[:10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1602, 2396], [258, 8725], [2827, 7812], [2601, 5271], [5200, 474], [60, 5612], [2108, 10], [220, 630], [177, 4685], [2459, 7442]] [0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E99ozst6ef4",
        "colab_type": "text"
      },
      "source": [
        "Create input Vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_HHp2EZ6hlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_target = Input((1,))\n",
        "input_context = Input((1,))\n",
        "\n",
        "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
        "target = embedding(input_target)\n",
        "target = Reshape((vector_dim, 1))(target)\n",
        "context = embedding(input_context)\n",
        "context = Reshape((vector_dim, 1))(context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0nQX6nY6lkx",
        "colab_type": "text"
      },
      "source": [
        "Setup a cosine similarity operation which will be output in a secondary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4vliR5h6mh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#similarity = merge.Add([target, context], mode='cos', dot_axes=0)\n",
        "# use dot - should be normalize True? https://stackoverflow.com/questions/51003027/computing-cosine-similarity-between-two-tensors-in-keras\n",
        "similarity = dot([target, context], axes=1, normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}